{"meta":{"title":"BL's Blog","subtitle":"Welcome To My World!","description":"BL的个人博客","author":"LiuBL","url":"https://selamangit.github.io/ZaNot.github.io","root":"/ZaNot.github.io/"},"pages":[{"title":"404","date":"2021-10-21T12:36:50.000Z","updated":"2021-10-21T12:37:16.798Z","comments":true,"path":"404/index.html","permalink":"https://selamangit.github.io/ZaNot.github.io/404/index.html","excerpt":"","text":""},{"title":"contact","date":"2021-10-21T12:33:16.000Z","updated":"2021-10-21T12:34:09.974Z","comments":true,"path":"contact/index.html","permalink":"https://selamangit.github.io/ZaNot.github.io/contact/index.html","excerpt":"","text":""},{"title":"categories","date":"2021-10-21T12:30:59.000Z","updated":"2021-10-21T12:31:42.531Z","comments":true,"path":"categories/index.html","permalink":"https://selamangit.github.io/ZaNot.github.io/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2021-10-21T12:32:28.000Z","updated":"2021-10-21T12:32:52.809Z","comments":true,"path":"about/index.html","permalink":"https://selamangit.github.io/ZaNot.github.io/about/index.html","excerpt":"","text":""},{"title":"friends","date":"2021-10-21T12:34:41.000Z","updated":"2021-10-21T12:34:55.714Z","comments":true,"path":"friends/index.html","permalink":"https://selamangit.github.io/ZaNot.github.io/friends/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-10-21T12:32:04.000Z","updated":"2021-10-21T12:32:21.063Z","comments":true,"path":"tags/index.html","permalink":"https://selamangit.github.io/ZaNot.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"TOPSIS法（优劣解距离法）","slug":"数学建模-TOPSIS法","date":"2022-09-09T02:34:17.117Z","updated":"2022-09-09T02:42:10.263Z","comments":true,"path":"2022/09/09/shu-xue-jian-mo-topsis-fa/","link":"","permalink":"https://selamangit.github.io/ZaNot.github.io/2022/09/09/shu-xue-jian-mo-topsis-fa/","excerpt":"","text":"TOPSIS法（优劣解距离法） TOPSIS法是一种常用的综合评价方法，其能充分利用原始数据的信息，其结果能精确地反映各评价方案之间的差距，这种方法==通过构造评价问题的正理想解和负理想解，即各指标的最优解和最劣解，通过计算每个方案到理想方案的相对贴近度，即靠近正理想解和远离负理想解的程度，来对方案进行排序==，从而选出最优方案。 方法和原理设多属性决策方案集为$\\boldsymbol{ D={d_1,d_2,…,d_m } }$，衡量方案优劣的属性变量为$\\boldsymbol{ \\ x_1\\ ,\\ x_2\\ ,…\\ ,x_n }$，这时方案集$\\boldsymbol{D}$中的每一个方案$\\boldsymbol{\\ d_i(i=1…m)}$的$\\boldsymbol{n}$个属性值构成的向量是$\\boldsymbol{ { a_{ i1 },…,\\ a_{ in }} }$，它作为$\\boldsymbol{n}$维空间中的一个点，能唯一表征方案$\\boldsymbol{\\ d_i }$，这些方案的属性共同构成决策矩阵。 基本概念正理想解正理想解$\\boldsymbol{C^*}$是一个方案集$\\boldsymbol{D}$中不存在的虚拟的最佳方案，它的每个属性值都是决策矩阵中该属性值的最优解。负理想解负理想解$\\boldsymbol{C^0}$则是虚拟的最差方案，它的每个属性值都是决策矩阵中该属性的最差值。 方法在$\\boldsymbol{n}$维空间中，将方案集$\\boldsymbol{D}$中的各备选方案$\\boldsymbol{d_i}$与正理想解$\\boldsymbol{\\ C^*}$和负理想解$\\boldsymbol{\\ C^0}$的距离进行比较，既靠近正理想解又远离负理想解的方案就是方案集$\\boldsymbol{D}$中的最优方案；并可以据此来排定各备选方案的优先序。 原理用TOPSIS法求解多属性决策问题的概念简单，只要在属性空间==定义适当的距离测度==就能通过欧氏距离来计算备选方案与理想解的距离。即用正理想解，又用负理想解的原因是仅仅使用正理想解时有时会出现某两个备选方案与正理想解的距离相同的情况，为了区分这两个方案的优劣，引入负理想解，并计算这两个方案与负理想解的距离，与正理想解距离相同的方案离负理想解远的为优。 TOPSIS算法步骤 用向量规划化的方法求得规范决策矩阵。设多属性决策问题的决策矩阵$\\boldsymbol{A=(a_{ij}){m\\times n}}$，规范化决策矩阵$\\boldsymbol{B=(b{ij}){m\\times n}}$，其中$$\\boldsymbol{b{ij}=\\frac{a_{ij}}{\\sqrt{\\sum_{i=1}^ma_{ij}^2}}(i=1,2,…,m;j=1,2,…,n)}$$ 构成加权规范阵$\\boldsymbol{C=(c_{ij}){m\\times n}}$，设由决策人给定各属性的权重向量为$\\boldsymbol{w=[w_1,w_2,…,w_n]^T}$，则$$\\boldsymbol{c{ij}=w_j\\cdot b_{ij}(i=1,2,…,m;j=1,2,…,n)}$$ 确定正理想解的第$\\boldsymbol{j}$个属性$\\boldsymbol{C^*_j}$和负理想解的第$\\boldsymbol{j}$个属性$\\boldsymbol{C^0_j}$ 计算各方案到正理想解与负理想解的距离，备选方案$\\boldsymbol{d_i}$到正理想解的距离为$$\\boldsymbol{s^*i=\\sqrt{\\sum{j=1}^n(c_{ij}-c_j^*)^2},(i=1,2,…,m)}$$备选方案$\\boldsymbol{d_i}$到负理想解的距离为$$\\boldsymbol{s_i^0=\\sqrt{\\sum^n_{j=1}(c_{ij}-c_j^0)^2}(i=1,2,…,m)}$$ 计算各方案的排队指标值（即综合评价指标），即$$\\boldsymbol{f_i^*=\\frac{s_i^0}{s_i^0+s_i^*}}$$ 按$\\boldsymbol{f_i^*}$由大到小排列方案的优劣次序 属性值的规范化属性值规范化处理的目的在进行决策是，一般要进行属性值的规范化处理，主要有三个作用： 属性值有多种类型，不同类型类型的属性放在同一个表中不便于直接从数值大小判断方案的优劣，因此需要对数据进行预处理，使得表中任意属性下性能越优的方案变换后的属性值越大。 非量纲化，多属性决策方法与评估的困难之一是属性间的不可公度性，即在属性值表中的每一类数具有不同的量纲，在用各种多属性决策方法进行分析评价时，需要排除量纲的选用对决策或评估结果的影响，这就是非量纲化。 归一化，属性值表中不同指标的属性值的数值差别很大们为了直观，更为了便于采用各种多属性决策ui评估方法进行评价，需要把属性值表中数值归一化处理，即把表中数值均变换到$\\boldsymbol{[0,1]}$区间上。 属性值类型 属性值类型 类型特点 效益型 属性值越大越好 成本型 属性值越小越好 区间型 属性值在某个区间内最好 中间型 属性值在中间的时候最好 规范化处理方法1.线性变换原始的决策矩阵$\\boldsymbol{A=(a_{ij}){m\\times n}}$，变换后的决策矩阵$\\boldsymbol{B=(b{ij}){m\\times n}(i=1,…,m;j=1,…,n)}$设$\\boldsymbol{a_j^{max}}$是决策矩阵第$\\boldsymbol{j}$列中的最大值，$\\boldsymbol{a_j^{min}}$是决策矩阵第$\\boldsymbol{j}$列中的最小值，若$\\boldsymbol{x_j}$为效益性属性，则$$\\boldsymbol{b{ij}=\\frac{a_{ij}}{a_j^{max}}}$$此时最差属性值不一定为0，最优属性值为1若$\\boldsymbol{x_j}$为成本型属性，则$$\\boldsymbol{b_{ij}=1-\\frac{a_{ij}}{a_j^{max}}}$$ 2.标准0-1变换为了是每个属性变换后的最优解为1，且最差值为0，可以进行标准0-1变换。对效益性属性$\\boldsymbol{x_j}$，令$$\\boldsymbol{b_{ij}=\\frac{a_{ij}-a^{min}_j}{a_j^{max}-a^{min}j}}$$对成本型属性$\\boldsymbol{x_j}$，令$$\\boldsymbol{\\frac{a_j^{max}-a{ij}}{a_j^{max}-a_j^{min}}}$$ 3.区间型属性的变换设给定的最优属性区间为$\\boldsymbol{[a_J^0,a_j^*]}$，$\\boldsymbol{a’j}$为无法容忍下限，$\\boldsymbol{a’’j}$为无法容忍上限，则$$\\boldsymbol{b{ij}=\\left{\\begin{array}{l} 1-\\frac{(a_j^0-a{ij})}{a_j^0-a_j’}&amp;,&amp;a’j\\le a{ij} \\le a^0_j\\ 1&amp;,&amp;a_j^0\\le a_{ij}\\le a_j^\\ 1-\\frac{a_{ij}-a_j^}{a’’j-a_j^*}&amp;,&amp;a_j^*&lt;a{ij}\\le a’’j\\ 0&amp;,&amp;其他\\end{array}\\right.}$$变换后的属性值$\\boldsymbol{b{ij}}$与原属性值$\\boldsymbol{a_{ij}}$之间的函数图形为一般梯形，当属性值最有区间的上下限相等时，最优区间退化为一个点时，函数图形退化为三角形 4.向量规范化规范化后，各方案的同一属性值的平方和为1，常用于计算各种方案与某种虚拟方案的欧氏距离，一般要先将属性值正向化，即利用上面的变换来实现，正向化后再规范化就能直观从属性值上表示出各方案再某一属性下的情况 5. 标准化处理在实际问题中，不同变量的测量单位往往是不一样的。为了消除变量的量纲效应，使每个变量都具有同等的表现力，数据分析中常对数据进行标准化处理，即$$\\boldsymbol{b_{ij}=\\frac{a_{ij}-\\bar{a}j}{s_j},i=1,2…,m;j=1,2,…,n}$$其中：$\\boldsymbol{\\bar{a}j=\\frac{1}{m}\\sum^m{i=1}a{ij}\\ ,s_j=\\sqrt{\\frac{1}{m-1}\\sum^m_{i=1}(a_{ij}-\\bar{a})^2},j=1,2,…,n}$ 代码实现： import numpy as np import proces_data import math def create_array(dic,require,require1,require2):#,require3 array_of_A = [] name = [] for i in dic.items(): temp = [] temp.append(i[1][require]) temp.append(i[1][require1]) temp.append(i[1][require2]) #temp.append(i[1][require3]) array_of_A.append(temp) name.append(i[0]) array_of_A = np.array(array_of_A) return array_of_A,name def Normalzation(stand_of_b): molecular = stand_of_b Denominator = np.sum(np.square(stand_of_b),axis=0) normal_of_b = molecular / np.sqrt(Denominator) return normal_of_b def Entropy_method(array_of_a): a = array_of_a molecular = a - np.min(a,axis=0) Denominator = np.max(a,axis=0) - np.min(a,axis=0) Positive_index = molecular / Denominator p = Positive_index / np.sum(Positive_index,axis=0) k = (1 / math.log(len(a))) result = np.zeros(shape=p.shape) for i in range(len(p)): for j in range(len(p[i])): if p[i][j] == 0: result[i][j] = 0 else: result[i][j] = p[i][j] * math.log(p[i][j]) Entropy = - k * (np.sum(result,axis=0) ) diff_Enr = 1 - Entropy weight = diff_Enr / np.sum(diff_Enr) #weight = weight[np.newaxis,:] return weight def weighted_array(weight,array_of_n): return array_of_n * weight def calculate(weighted_array): max = np.max(weighted_array,axis=0) min = np.min(weighted_array,axis=0) molecular = np.sqrt(np.sum(np.square(weighted_array-min))) Denominator = molecular + np.sqrt(np.sum(np.square(weighted_array-max),axis=1)) result = molecular / Denominator return result","categories":[{"name":"数学建模学习笔记","slug":"数学建模学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数学建模学习笔记","slug":"数学建模学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"author":"LiuBL"},{"title":"YOLO-V3","slug":"YOLO-V3","date":"2022-09-08T13:43:05.569Z","updated":"2022-09-09T01:43:59.310Z","comments":true,"path":"2022/09/08/yolo-v3/","link":"","permalink":"https://selamangit.github.io/ZaNot.github.io/2022/09/08/yolo-v3/","excerpt":"","text":"YOLO-V3 YOLO-V3是比较完善的一个目标检测的框架，它经过一代一代的发展，一步步地完善，接下来介绍一些YOLO-V3在V2的基础上进行了哪些的改进，下图为我训练出来模型的检测效果 先验框的更新在V2中，先验框是由K-mean聚类算法得出的5种不同尺寸的先验框，在V3中考虑到Darknet-53会输出三种尺寸的预测，因此选取先验框时应该选取的是3的倍数，最终选择了9种先验框，每种尺寸都有三种不同形状的先验框。 先验框的选择仍然是通过K-mean聚类出9种，这9种聚类得出的先验框之间不是没有关系的，其中每种尺寸的特征图都有其专属的三种基本相同大小的先验框用来预测，例如：13×13尺寸的特征图中会有三种较大的先验框用来预测，26×26的特征图会有较中等的三种先验框用来预测，52×52的特征图则会有三种较小的先验框用来预测 K-mean聚类的距离度量方法仍然是利用IOU来计算，这些先验框都是根据groundtruth的数据聚类出来的 正负样本分配在前面的学习，我们已经知道除了要计算有物体的网格的损失，还要计算不含物体的网格的损失，因此就有了正负样本的说法 根据上面聚类出来的9种先验框，计算出三个尺寸所对应的三个先验框与groundtruth的IOU，具体做法为：每个尺寸用对应的三种大小的先验框去与这个尺寸的groundtruth计算IOU，如果计算出来的先验框的IOU值存在大于阈值，就标记这个先验框为这个尺寸下的正样本，并设该尺寸下IOU大于阈值的先验框的置信度为1；如果计算出来的IOU都不大于阈值，那就选取计算出来IOU最大的先验框作为正样本 以上的两种方法就是筛选正样本的方法，如果不满足这两种方法，剩下的先验框就标记为负样本。标记正负样本只会参与到置信度损失的计算中，不会参与到预测类别以及bbox位置预测的损失计算中 正负样本分配也需要遵循一些准则，那就是尽量使得正样本数和负样本数基本相等，这是为了让模型在预测时不会出现大量漏检的现象，按照我的理解就是：先验框计算出来的IOU只要大于阈值，就说明这个先验框是能检测到物体的，正样本数越多就说明这个区域内存在物体的概率更大，也就不至于出现大量漏检的现象 网络结构上的改进首先来看看它的主体网络：Darknet-53的结构图 网络结构的改变是YOLO-V3中比较大改进的地方 在YOLO-V2中使用的主体网络结构为： Darknet-19，在Darknet-19中使用卷积层来提取特征图，通过池化层来获取降低了特征维度后的特征图（具体结构请去看YOLO-V2），在经过Darknet-19全部提取完特征图之后，再将上一层池化之前的结果经过reorg之后再合并到一起，来实现特征融合，将感受野较小的特征图融合到感受野较大的特征图上面，可以使得较少的目标不至于丢失 在YOLO-V3中的Darknet-53不同于V2中的网络，它除了使用卷积层来提取特征图之外，还使用了卷积层来降低特征图的维度，而不是池化层。仔细想想确实可以用卷积层来代替池化层来实现特征的降维，而且这个网络的主要成分还是残差块$\\left(\\text{resn}\\right)$，残差块是由一层特征降维的卷积层和 $\\text{2n}$ 个提取特征图的卷积层组成，但是在每 $\\text{2}$ 个提取特征图的卷积层之后，都要将输出的 $\\text{conv}$ 和输入的 $\\text{conv}$ 相加 在Darknet-53的卷积降维中会提取出大小不同的特征图，以输入图片大小为416×416为例，网络会将其降维为13×13、26×26、52×52等尺寸。若是像V2的做法，会将尺寸较大的特征图拆分之后融入尺寸较大的特征图中去，但想想V2的做法真的合适吗？将检测小物体（感受野较小）的特征图融合到检测大物体（感受野较大）的特征图中去，可能会将本来检测大物体的功能减弱或者检测小物体的功能减弱 那么V3中是怎么做的呢？它将最后提取出来的三种不同尺寸的特征图进行处理，将尺寸小的特征图进行上采样，还原到上一层的尺寸大小，然后将该特征图拼接到上一层的特征图中去，例如：将13×13的特征图经过上采样变为26×26大小的特征图，然后拼接到26×26尺寸的特征图中去。 我的理解是：在Darknet-53中通过残差块提取的特征图中已包含了前面的信息，小物体的信息也提取出来了，经过参数的优化小物体的检测会得到一定效果，此时应该向感受野大的特征图获取一些全局信息（个人理解仍待考证） decode解码由于网络输出的预测值的范围是0~1之间，也就是说每一个网格输出的bbox的位置预测都是都被限制在这个网格内，这正是YOLO的划分，每个网格都来预测是否有物体中心点。而decode就是用来将特征预测值都还原到相应尺寸下的尺寸，具体做法就是：将坐标预测值（0 ~ 1之间）与网格左上角的坐标相加获得该bbox在相应尺寸下的坐标，以及将预测的宽高乘上相应的尺寸，从而获得bbox的宽高，这样就可以计算出bbox与groundtruth的IOU，那么bbox的置信度就得出来了，就可以预测此处是否存在物体了 处理解码后的bbox网络输出的预测值经过解码之后，会获取所有尺寸下的网格所输出三个预测框bbox的信息，这些预测出来的bbox有可能存在左上角的坐标比右下角坐标大的情况，或者bbox不存在的情况，在检测目标时需要将这些预测出来的bbox信息去除先，保留下那些bbox信息存在且不异常的 NMS筛选bbox（在预测阶段）NMS（Non-Maximum Suppression）抑制不是极大值的元素，说白了就是去除哪些重叠率较高并且score评分较低的bbox 处理了解码后的bbox，会保留下一些bbox的信息，但是不是每个bbox都去负责呢？仔细想想每个网格（对应尺寸下）会输出三个bbox，假如处理后的bbox信息都被保留下来了，那么一个物体就会有三个预测框去框住它，这样的展示效果就不太好了。 对于一个网格内的bbox，需要的是保存下那个能最好地检测出物体的那个bbox，就要用到NMS方法了方法就是： 首先判断边界框的数目是否大于0，如果不是大于0，说明没有bbox的信息被保留下来，就去检测下一个网格，直到所有尺寸下的网格都被检测完 在保留bbox时，会给每个bbox一个评分，对于一个网格内的bbox，根据评分，为这些bbox排序，（不放回的取）取出评分最大的那个bbox 把这个评分最高的bbox与同一网格下的其他bbox计算IOU，这是为了判断评分最大的bbox与同一网格下的其他bbox的重叠率，去除那些重叠率过高的，取到剩下的bbox为0为止 GIOU（广义IOU）这是一种优化边界框的方法，GIOU是衡量bbox与实际框之间对齐和重叠情况的一个指标IOU计算公式：$$\\text{IOU }= \\left|\\frac{A \\cap B}{A\\cup B}\\right|$$GIOU的计算公式：$$\\text{GIOU }= \\left| \\frac{C/(A\\cup B)}{C} \\right|$$","categories":[{"name":"深度学习笔记","slug":"深度学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"深度学习笔记","slug":"深度学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"author":"LiuBL"},{"title":"动态规划理论","slug":"动态规划","date":"2021-10-21T15:01:16.139Z","updated":"2021-10-21T15:07:24.131Z","comments":true,"path":"2021/10/21/dong-tai-gui-hua/","link":"","permalink":"https://selamangit.github.io/ZaNot.github.io/2021/10/21/dong-tai-gui-hua/","excerpt":"","text":"1. 什么是动态规划在现实生活中，有一类活动的过程，由于它的特殊性，可将过程分成若干个互相联系的阶段，在它的每一阶段都需要作出决策，从而使整个过程达到最好的活动效果。因此各个阶段决策的选取不能任意确定，它依赖于当前面临的状态，又影响以后的发展。当各个阶段决策确定后，就组成一个决策序列，因而也就确定了整个过程的一条活动路线。 这种把一个问题看作是一个前后关联具有链状结构的多阶段过程就称为多阶段决策过程，这种问题称为多阶段决策问题。在多阶段决策问题中，各个阶段采取的决策，一般来说是与时间有关的，决策依赖于当前状态，又随即引起状态的转移，一个决策序列就是在变化的状态中产生出来的，故有“动态”的含义，称这种解决多阶段决策最优化的过程为动态规划方法。 2. 动态规划的思想动态规划与分治法类似，都是==把大问题拆分成小问题==，通过寻找大问题与小问题的递推关系，解决一个个小问题，最终达到解决原问题的效果。但不同的是，分治法在子问题和子子问题等上被重复计算了很多次，而动态规划则具有记忆性，通过填写表把所有已经解决的子问题答案纪录下来，在==新问题里需要用到的子问题可以直接提取，避免了重复计算==，从而节约了时间，所以在问题满足最优性原理之后，用动态规划解决问题的核心就在于==填表==，表填写完毕，最优解也就找到。 所谓的最优性原理是指：多阶段决策过程的最优决策序列具有这样的性质：不论初始状态和初始决策如何，对于前面决策所造成的某一状态而言（当前出发点），其后各阶段的决策序列必须构成最优策略。 3. 动态规划的作用动态规划实质上是一种以空间换时间的技术，它在实现的过程中，不得不存储产生过程中的各种状态，所以它的空间复杂度要大于其他的算法。选择动态规划算法是因为动态规划算法在空间上可以承受，而搜索算法在时间上却无法承受，所以我们舍空间而取时间 Questions动态规划怎么保证每个子问题的决策都是对最优结果的最好的决策呢？动态规划记住的状态一定是最优的吗？","categories":[{"name":"算法学习笔记","slug":"算法学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"算法学习笔记","slug":"算法学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/tags/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"author":"LiuBL"}],"categories":[{"name":"数学建模学习笔记","slug":"数学建模学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"深度学习笔记","slug":"深度学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"算法学习笔记","slug":"算法学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数学建模学习笔记","slug":"数学建模学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"深度学习笔记","slug":"深度学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"算法学习笔记","slug":"算法学习笔记","permalink":"https://selamangit.github.io/ZaNot.github.io/tags/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]}